# ğŸ“š Carpeta RAG - Documentos para Willay

Esta carpeta contiene los documentos PDF que Willay utilizarÃ¡ como base de conocimiento.

## ğŸ“– Â¿QuÃ© es RAG?

**RAG (Retrieval-Augmented Generation)** es una tÃ©cnica que permite a Willay:
- Buscar informaciÃ³n relevante en tus documentos
- Usar esa informaciÃ³n para responder preguntas con mayor precisiÃ³n
- Citar las fuentes (archivo y pÃ¡gina) de donde obtuvo la informaciÃ³n

## ğŸš€ CÃ³mo usar

### 1. Agregar documentos PDF

Simplemente coloca tus archivos PDF en esta carpeta:

```
rag/
â”œâ”€â”€ matematicas_basicas.pdf
â”œâ”€â”€ fisica_teoria.pdf
â”œâ”€â”€ programacion_python.pdf
â””â”€â”€ ...
```

### 2. Indexar los documentos

Tienes dos opciones:

**OpciÃ³n A - Script CLI (recomendado):**
```cmd
cd backend
python rag_cli.py index
```

**OpciÃ³n B - API REST:**
```cmd
curl -X POST http://127.0.0.1:8000/rag/index
```

### 3. Usar RAG en el chat

En el frontend, activa el toggle **"Usar RAG"** antes de hacer tu pregunta. Willay buscarÃ¡ automÃ¡ticamente informaciÃ³n relevante en los documentos indexados.

## ğŸ› ï¸ Comandos CLI

```cmd
# Indexar documentos
python rag_cli.py index

# Forzar re-indexaciÃ³n completa
python rag_cli.py index --force

# Ver estadÃ­sticas
python rag_cli.py stats

# Listar documentos indexados
python rag_cli.py list

# Limpiar Ã­ndice
python rag_cli.py clear

# Modo observador (auto-reindex cuando hay cambios)
python rag_cli.py watch
```

## ğŸ“Š Â¿QuÃ© sucede al indexar?

1. **ExtracciÃ³n**: Se extrae el texto de cada PDF pÃ¡gina por pÃ¡gina
2. **CachÃ©**: El texto se guarda en `backend/rag_engine/cache/` para no volver a procesar
3. **Chunking**: El texto se divide en fragmentos de ~800 caracteres
4. **Embeddings**: Cada fragmento se convierte en un vector usando Ollama (`nomic-embed-text`)
5. **Almacenamiento**: Los vectores se guardan en `backend/rag_engine/vector_store/` (ChromaDB)

## âš™ï¸ ConfiguraciÃ³n avanzada

Puedes ajustar parÃ¡metros en `backend/app.py`:

```python
rag_engine = RAGEngine(
    pdf_dir="rag",
    embedding_model="nomic-embed-text",  # o "mxbai-embed-large"
    chunk_size=800,                      # TamaÃ±o de chunks
    chunk_overlap=200                    # Overlap entre chunks
)
```

## ğŸ“ Notas importantes

- **Privacidad**: Todo el procesamiento es local, los documentos nunca salen de tu PC
- **Modelo de embeddings**: AsegÃºrate de tener `nomic-embed-text` en Ollama:
  ```cmd
  ollama pull nomic-embed-text
  ```
- **Rendimiento**: La indexaciÃ³n puede tardar varios minutos dependiendo del tamaÃ±o de los PDFs
- **CachÃ©**: Los archivos `.txt` en `cache/` aceleran la re-indexaciÃ³n

## ğŸ› SoluciÃ³n de problemas

### "No se encontraron PDFs"
- Verifica que los archivos tengan extensiÃ³n `.pdf`
- AsegÃºrate de estar en el directorio correcto

### "Error generando embeddings"
- Verifica que Ollama estÃ© corriendo: `curl http://127.0.0.1:11434`
- Instala el modelo: `ollama pull nomic-embed-text`

### La indexaciÃ³n es muy lenta
- Es normal, cada pÃ¡gina genera mÃºltiples embeddings
- Puedes reducir `chunk_size` para menos chunks (menos precisiÃ³n)

### No encuentra informaciÃ³n en el chat
- Verifica que RAG estÃ© activado (toggle en frontend)
- Aumenta `rag_n_results` para buscar mÃ¡s contexto
- Re-indexa con `--force` si modificaste los PDFs

## ğŸ¯ Mejores prÃ¡cticas

âœ… **DO:**
- Usar PDFs con texto seleccionable (no escaneos de imÃ¡genes)
- Organizar documentos por materia
- Re-indexar despuÃ©s de agregar nuevos PDFs
- Probar con preguntas especÃ­ficas del contenido

âŒ **DON'T:**
- No subas documentos confidenciales si compartes el proyecto
- No uses PDFs con DRM o protecciÃ³n de copia
- No esperes que funcione con PDFs puramente grÃ¡ficos

---

**ğŸ’¡ Tip**: Prueba hacer preguntas como:
- "Â¿QuÃ© dice el documento sobre [tema]?"
- "Resume la pÃ¡gina 5 del archivo [nombre]"
- "Explica el concepto de [X] segÃºn los documentos"

Willay citarÃ¡ automÃ¡ticamente las fuentes cuando use informaciÃ³n de los PDFs.
