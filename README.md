# ğŸ“ Willay - Asistente AcadÃ©mico con IA Local

**Willay** es un chatbot acadÃ©mico inteligente que utiliza Ollama para ejecutar modelos de lenguaje localmente en Windows, con streaming en tiempo real y memoria conversacional.

---

## âœ¨ CaracterÃ­sticas

- ğŸ¤– **IA Local con Ollama**: Usa modelos LLM (llama3.2, llama3, llama2) sin necesidad de APIs externas
- ğŸ’¬ **Streaming en tiempo real**: Respuestas token por token, como ChatGPT
- ğŸ§  **Memoria conversacional**: Mantiene contexto de la conversaciÃ³n con sistema de sesiones
- ğŸ“š **Prompts acadÃ©micos especializados**: MatemÃ¡ticas, FÃ­sica, ProgramaciÃ³n, Historia, Literatura, QuÃ­mica
- ğŸ“– **Historial persistente**: Guarda y recupera conversaciones anteriores
- âš™ï¸ **Panel de administrador**: Logs en tiempo real, estadÃ­sticas de uso
- ğŸ¨ **UI moderna y responsive**: DiseÃ±o limpio con sidebar desplegable
- ğŸ”§ **Configurable**: Selector de modelo, control de temperatura

---

## ğŸ“‹ Requisitos Previos

- **Windows 10/11**
- **Python 3.8+** instalado y en PATH
- **Ollama para Windows** ([descargar aquÃ­](https://ollama.com/download/windows))
- Navegador web moderno (Chrome, Edge, Firefox)

---

## ğŸš€ InstalaciÃ³n

### 1. Clonar el repositorio

```bash
git clone https://github.com/Musgus/Willay.git
cd Willay
```

### 2. Instalar y configurar Ollama

```cmd
# Ollama ya deberÃ­a estar corriendo tras instalarlo
# Verifica que estÃ© activo:
curl http://127.0.0.1:11434/api/tags

# Descarga el modelo llama3.2 (recomendado):
ollama pull llama3.2
```

### 3. Configurar el backend

Desde la carpeta `backend/`, ejecuta:

**OpciÃ³n A - Usando run.bat (doble clic):**
```cmd
cd backend
run.bat
```

**OpciÃ³n B - PowerShell:**
```powershell
cd backend
powershell -ExecutionPolicy Bypass -File run.ps1
```

**OpciÃ³n C - Manual:**
```cmd
cd backend
python -m venv .venv
.venv\Scripts\activate
pip install -r requirements.txt
uvicorn app:app --host 127.0.0.1 --port 8000
```

> âš ï¸ Si Windows Firewall solicita permiso, **acepta** para permitir conexiones locales.

---

## ğŸ¯ Uso

1. **Inicia el backend** (ver paso 3 arriba). DeberÃ­as ver:
   ```
   INFO:     Uvicorn running on http://127.0.0.1:8000
   ```

2. **Abre `index.html`** en tu navegador (doble clic o arrastra a la ventana del navegador)

3. **Â¡Listo!** Puedes:
   - Seleccionar un **prompt acadÃ©mico** en el sidebar (MatemÃ¡ticas, FÃ­sica, etc.)
   - Escribir tu pregunta directamente
   - Ajustar **modelo** y **temperatura** segÃºn necesites
   - Ver el **historial** de conversaciones en el sidebar
   - Acceder al **Panel Admin** (âš™ï¸) para ver logs y estadÃ­sticas

---

## ğŸ“ Estructura del Proyecto

```
Willay/
â”œâ”€â”€ index.html          # Interfaz principal del chatbot
â”œâ”€â”€ style.css           # Estilos responsive con sidebar
â”œâ”€â”€ script.js           # LÃ³gica frontend: streaming, historial, admin
â”œâ”€â”€ install.sh          # Instalador automÃ¡tico para Ubuntu Server
â”œâ”€â”€ UBUNTU_DEPLOYMENT.md # GuÃ­a completa de despliegue en Ubuntu
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py          # FastAPI con streaming y sesiones
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ run.bat         # Script de inicio (Windows)
â”‚   â”œâ”€â”€ run.ps1         # Script de inicio (PowerShell)
â”‚   â”œâ”€â”€ run.sh          # Script de inicio (Ubuntu/Linux)
â”‚   â”œâ”€â”€ willay-backend.service  # Archivo systemd para Ubuntu
â”‚   â”œâ”€â”€ rag_cli.py      # CLI para gestiÃ³n RAG
â”‚   â””â”€â”€ rag_engine/     # Motor RAG completo
â””â”€â”€ README.md
```

---

## ğŸ› ï¸ TecnologÃ­as

### Frontend (Vanilla)
- HTML5 + CSS3 (sin frameworks)
- JavaScript puro con Fetch API
- LocalStorage para persistencia

### Backend
- **FastAPI** - Framework web asÃ­ncrono
- **Uvicorn** - Servidor ASGI
- **httpx** - Cliente HTTP asÃ­ncrono para Ollama
- **Pydantic v2** - ValidaciÃ³n de datos

### IA
- **Ollama** - Motor de modelos LLM local
- Modelos soportados: llama3.2, llama3, llama2

---

## âš™ï¸ ConfiguraciÃ³n

### Cambiar modelo por defecto
Edita `index.html`, lÃ­nea 39:
```html
<option value="llama3.2" selected>llama3.2</option>
```

### Ajustar lÃ­mite de tokens
Edita `backend/app.py`, lÃ­nea 17:
```python
MAX_RESPONSE_CHARS = 500  # Caracteres mÃ¡ximos por respuesta
```

### Modificar temperatura por defecto
Edita `index.html`, lÃ­nea 44:
```html
<input type="range" id="temperatureRange" ... value="0.3" ...>
```

---

## ï¿½ Sistema RAG (Â¡Disponible Ahora!)

**RAG (Retrieval-Augmented Generation)** ya estÃ¡ implementado y funcional. CaracterÃ­sticas:

- âœ… **Carga de PDFs**: Sube documentos y Willay los usa como fuente de conocimiento
- âœ… **BÃºsqueda vectorial**: Embeddings locales con ChromaDB + Ollama
- âœ… **IndexaciÃ³n automÃ¡tica**: Procesa PDFs, extrae texto, genera embeddings
- âœ… **CitaciÃ³n de fuentes**: Willay menciona archivo y pÃ¡gina de donde obtiene informaciÃ³n
- âœ… **Panel de gestiÃ³n**: Interfaz web para subir, indexar y eliminar documentos
- âœ… **CLI incluido**: Script para indexar desde terminal
- âœ… **100% local**: Todo el procesamiento en tu PC, sin APIs externas

### ğŸš€ ConfiguraciÃ³n RAG

**1. Instalar modelo de embeddings:**
```cmd
ollama pull nomic-embed-text
```

**2. Reinstalar dependencias con soporte RAG:**
```cmd
cd backend
.venv\Scripts\activate
pip install -r requirements.txt
```

**3. Colocar PDFs en la carpeta `rag/`**

**4. Indexar documentos:**
```cmd
cd backend
python rag_cli.py index
```

**5. En el frontend, activar el toggle "ğŸ“š Usar RAG"**

ğŸ“– **GuÃ­a completa**: Ver [SETUP_RAG.md](SETUP_RAG.md) para instrucciones detalladas.

### Comandos RAG disponibles

```cmd
# Indexar documentos
python backend/rag_cli.py index

# Ver estadÃ­sticas
python backend/rag_cli.py stats

# Listar archivos indexados
python backend/rag_cli.py list

# Limpiar Ã­ndice
python backend/rag_cli.py clear

# Modo observador (auto-reindex)
python backend/rag_cli.py watch
```

---

## ğŸ› SoluciÃ³n de Problemas

### Error: `ModuleNotFoundError: No module named 'httpx'`
**SoluciÃ³n**: Instala las dependencias con `pip install -r requirements.txt` dentro del venv.

### Error: `NetworkError when attempting to fetch resource`
**SoluciÃ³n**: 
1. Verifica que el backend estÃ© corriendo en `http://127.0.0.1:8000`
2. Revisa que CORS permita `null` (ya configurado)

### Error: `Ollama no disponible`
**SoluciÃ³n**:
1. Verifica que Ollama estÃ© corriendo: `curl http://127.0.0.1:11434/api/tags`
2. Si no, inicia Ollama o reinicia el servicio de Windows

### El streaming no funciona
**SoluciÃ³n**: Algunos navegadores bloquean streaming desde `file://`. Usa un servidor local:
```cmd
python -m http.server 5500
```
Luego abre `http://localhost:5500`

---

## ï¿½ Despliegue en Ubuntu Server

Willay ahora soporta despliegue completo en Ubuntu Server con instalaciÃ³n automÃ¡tica.

### InstalaciÃ³n RÃ¡pida en Ubuntu

```bash
# Clonar repositorio
git clone https://github.com/Musgus/Willay.git
cd Willay

# Dar permisos de ejecuciÃ³n
chmod +x install.sh

# Ejecutar instalador (instala Ollama, Nginx, crea servicios systemd)
sudo ./install.sh
```

El instalador automÃ¡ticamente:
- âœ… Instala Python, Nginx y dependencias
- âœ… Instala y configura Ollama
- âœ… Descarga modelos (llama3.2, nomic-embed-text)
- âœ… Crea servicio systemd para el backend
- âœ… Configura Nginx como reverse proxy
- âœ… Inicia todos los servicios

DespuÃ©s de la instalaciÃ³n:
- Frontend: `http://TU_IP_SERVIDOR`
- API: `http://TU_IP_SERVIDOR:8000`

### GestiÃ³n del Servicio

```bash
# Ver logs en tiempo real
sudo journalctl -u willay-backend -f

# Reiniciar servicio
sudo systemctl restart willay-backend

# Estado
sudo systemctl status willay-backend
```

ğŸ“– **GuÃ­a completa**: Ver [UBUNTU_DEPLOYMENT.md](UBUNTU_DEPLOYMENT.md) para instrucciones detalladas, configuraciÃ³n avanzada, HTTPS, monitoreo y troubleshooting.

---

## ï¿½ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la licencia MIT. Consulta el archivo `LICENSE` para mÃ¡s detalles.

---

## ğŸ‘¨â€ğŸ’» Autor

Desarrollado por **Musgus**  
GitHub: [@Musgus](https://github.com/Musgus)

---

## ğŸ¤ Contribuciones

Â¡Las contribuciones son bienvenidas! Por favor:

1. Fork el proyecto
2. Crea tu rama de caracterÃ­stica (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add: nueva caracterÃ­stica'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

---

## ğŸ“ Notas

- Este proyecto NO envÃ­a datos a servicios externos; todo se ejecuta localmente.
- Compatible con **Windows** (scripts .bat/.ps1) y **Ubuntu Server** (scripts .sh + systemd).
- Requiere ~8GB RAM para despliegue en servidor con Ollama.
- Para desarrollo local Windows, 4GB RAM es suficiente.

---

**ğŸ“ Hecho con â¤ï¸ para estudiantes que buscan privacidad y control sobre su asistente IA.**
