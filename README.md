# ğŸ“ Willay - Asistente AcadÃ©mico con IA Local

**Willay** es un chatbot acadÃ©mico inteligente que utiliza Ollama para ejecutar modelos de lenguaje localmente en Windows, con streaming en tiempo real y memoria conversacional.

---

## âœ¨ CaracterÃ­sticas

- ğŸ¤– **IA Local con Ollama**: Usa modelos LLM (llama3.2, llama3, llama2) sin necesidad de APIs externas
- ğŸ’¬ **Streaming en tiempo real**: Respuestas token por token, como ChatGPT
- ğŸ§  **Memoria conversacional**: Mantiene contexto de la conversaciÃ³n con sistema de sesiones
- ğŸ“š **Prompts acadÃ©micos especializados**: MatemÃ¡ticas, FÃ­sica, ProgramaciÃ³n, Historia, Literatura, QuÃ­mica
- ğŸ“– **Historial persistente**: Guarda y recupera conversaciones anteriores
- âš™ï¸ **Panel de administrador**: Logs en tiempo real, estadÃ­sticas de uso
- ğŸ¨ **UI moderna y responsive**: DiseÃ±o limpio con sidebar desplegable
- ğŸ”§ **Configurable**: Selector de modelo, control de temperatura

---

## ğŸ“‹ Requisitos Previos

- **Windows 10/11**
- **Python 3.8+** instalado y en PATH
- **Ollama para Windows** ([descargar aquÃ­](https://ollama.com/download/windows))
- Navegador web moderno (Chrome, Edge, Firefox)

---

## ğŸš€ InstalaciÃ³n

### 1. Clonar el repositorio

```bash
git clone https://github.com/Musgus/Willay.git
cd Willay
```

### 2. Instalar y configurar Ollama

```cmd
# Ollama ya deberÃ­a estar corriendo tras instalarlo
# Verifica que estÃ© activo:
curl http://127.0.0.1:11434/api/tags

# Descarga el modelo llama3.2 (recomendado):
ollama pull llama3.2
```

### 3. Configurar el backend

Desde la carpeta `backend/`, ejecuta:

**OpciÃ³n A - Usando run.bat (doble clic):**
```cmd
cd backend
run.bat
```

**OpciÃ³n B - PowerShell:**
```powershell
cd backend
powershell -ExecutionPolicy Bypass -File run.ps1
```

**OpciÃ³n C - Manual:**
```cmd
cd backend
python -m venv .venv
.venv\Scripts\activate
pip install -r requirements.txt
uvicorn app:app --host 127.0.0.1 --port 8000
```

> âš ï¸ Si Windows Firewall solicita permiso, **acepta** para permitir conexiones locales.

---

## ğŸ¯ Uso

1. **Inicia el backend** (ver paso 3 arriba). DeberÃ­as ver:
   ```
   INFO:     Uvicorn running on http://127.0.0.1:8000
   ```

2. **Abre `index.html`** en tu navegador (doble clic o arrastra a la ventana del navegador)

3. **Â¡Listo!** Puedes:
   - Seleccionar un **prompt acadÃ©mico** en el sidebar (MatemÃ¡ticas, FÃ­sica, etc.)
   - Escribir tu pregunta directamente
   - Ajustar **modelo** y **temperatura** segÃºn necesites
   - Ver el **historial** de conversaciones en el sidebar
   - Acceder al **Panel Admin** (âš™ï¸) para ver logs y estadÃ­sticas

---

## ğŸ“ Estructura del Proyecto

```
Willay/
â”œâ”€â”€ index.html          # Interfaz principal del chatbot
â”œâ”€â”€ style.css           # Estilos responsive con sidebar
â”œâ”€â”€ script.js           # LÃ³gica frontend: streaming, historial, admin
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py          # FastAPI con streaming y sesiones
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ run.bat         # Script de inicio (Windows)
â”‚   â””â”€â”€ run.ps1         # Script de inicio (PowerShell)
â””â”€â”€ README.md
```

---

## ğŸ› ï¸ TecnologÃ­as

### Frontend (Vanilla)
- HTML5 + CSS3 (sin frameworks)
- JavaScript puro con Fetch API
- LocalStorage para persistencia

### Backend
- **FastAPI** - Framework web asÃ­ncrono
- **Uvicorn** - Servidor ASGI
- **httpx** - Cliente HTTP asÃ­ncrono para Ollama
- **Pydantic v2** - ValidaciÃ³n de datos

### IA
- **Ollama** - Motor de modelos LLM local
- Modelos soportados: llama3.2, llama3, llama2

---

## âš™ï¸ ConfiguraciÃ³n

### Cambiar modelo por defecto
Edita `index.html`, lÃ­nea 39:
```html
<option value="llama3.2" selected>llama3.2</option>
```

### Ajustar lÃ­mite de tokens
Edita `backend/app.py`, lÃ­nea 17:
```python
MAX_RESPONSE_CHARS = 500  # Caracteres mÃ¡ximos por respuesta
```

### Modificar temperatura por defecto
Edita `index.html`, lÃ­nea 44:
```html
<input type="range" id="temperatureRange" ... value="0.3" ...>
```

---

## ğŸ”® PrÃ³ximas CaracterÃ­sticas (RAG)

- ğŸ“š **RAG (Retrieval-Augmented Generation)**: Carga PDFs, TXT y documentos para entrenar a Willay con tu propio "rack de libros"
- ğŸ” **BÃºsqueda vectorial**: Embeddings con ChromaDB o FAISS
- ğŸ“Š **IndexaciÃ³n de URLs**: Scraping y entrenamiento desde sitios web
- ğŸ“ **Modos de entrenamiento**: Contexto especÃ­fico por materia

---

## ğŸ› SoluciÃ³n de Problemas

### Error: `ModuleNotFoundError: No module named 'httpx'`
**SoluciÃ³n**: Instala las dependencias con `pip install -r requirements.txt` dentro del venv.

### Error: `NetworkError when attempting to fetch resource`
**SoluciÃ³n**: 
1. Verifica que el backend estÃ© corriendo en `http://127.0.0.1:8000`
2. Revisa que CORS permita `null` (ya configurado)

### Error: `Ollama no disponible`
**SoluciÃ³n**:
1. Verifica que Ollama estÃ© corriendo: `curl http://127.0.0.1:11434/api/tags`
2. Si no, inicia Ollama o reinicia el servicio de Windows

### El streaming no funciona
**SoluciÃ³n**: Algunos navegadores bloquean streaming desde `file://`. Usa un servidor local:
```cmd
python -m http.server 5500
```
Luego abre `http://localhost:5500`

---

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la licencia MIT. Consulta el archivo `LICENSE` para mÃ¡s detalles.

---

## ğŸ‘¨â€ğŸ’» Autor

Desarrollado por **Musgus**  
GitHub: [@Musgus](https://github.com/Musgus)

---

## ğŸ¤ Contribuciones

Â¡Las contribuciones son bienvenidas! Por favor:

1. Fork el proyecto
2. Crea tu rama de caracterÃ­stica (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add: nueva caracterÃ­stica'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

---

## ğŸ“ Notas

- Este proyecto NO envÃ­a datos a servicios externos; todo se ejecuta localmente.
- Compatible Ãºnicamente con Windows (por ahora).
- Requiere ~4GB RAM mÃ­nimo para modelos llama3.2.

---

**ğŸ“ Hecho con â¤ï¸ para estudiantes que buscan privacidad y control sobre su asistente IA.**
